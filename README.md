# Hi there, I'm Shayan ðŸ‘‹

## Aspiring AI Safety Researcher

I'm passionate about ensuring advanced AI systems are developed safely, robustly, and aligned with human values. Currently focusing on building my knowledge and skills in this critical field.

### ðŸ”¬ Research Interests

- **AI Alignment**: Working to ensure AI systems reliably pursue intended goals
- **Interpretability**: Making AI systems more transparent and understandable
- **Robustness**: Building systems that perform reliably under distribution shifts
- **Value Learning**: Developing techniques to learn and respect human preferences
- **AI Governance**: Exploring policy frameworks for responsible AI development

### ðŸŒ± I'm currently learning

- Deep reinforcement learning from human feedback (RLHF)
- Causal interpretability methods
- Mechanistic interpretability techniques
- Formal verification approaches for neural networks

### ðŸ“š Background

- Bachelor of Science, Computer Science
- Summer 2024 Research Intern @ SprintML lab, CISPA
- Research Fellow @ Fatima Fellowship

### ðŸ“« Connect with me

- [Twitter/X](https://x.com/ShayanShamsi11)
- [LinkedIn](https://www.linkedin.com/in/muhammad-shayan-shamsi/)
- Email: shayans210@gmail.com

### ðŸ“– Recommended Reading

If you're interested in AI safety, check out these resources:
- [Center for AI Safety](https://www.safe.ai/)
- [AI Alignment Forum](https://alignmentforum.org/)

---

> "The development of full artificial intelligence could spell the end of the human race... or it could be the best thing that ever happened to us. We just have to be sure it's the latter." â€” Stephen Hawking
